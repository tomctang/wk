{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f34252",
   "metadata": {},
   "source": [
    "# Using Bayesian optimization and multimodal data fusion to build a Algo Trading Framework\n",
    "Outline of the steps to be followed to implement a trading strategy that uses Bayesian optimization and multimodal data fusion:\n",
    "\n",
    "-    Collect and preprocess the data: Gather the relevant data sources, such as price data, news articles, social media posts, and economic indicators. Clean, normalize, and transform the data as needed to create a unified representation that captures the relevant information.\n",
    "\n",
    "-    Define the objective function: Specify the performance metric that you want to optimize, such as the Sharpe ratio or the profit and loss (P&L). You can use the objective function to evaluate the performance of the trading strategy on a hold-out test set.\n",
    "\n",
    "-    Define the machine learning model: Choose a suitable model architecture that can integrate multiple sources of data and capture the relevant patterns and trends. For example, you can use a CNN or a recurrent neural network (RNN) for text data, and a feedforward network or an LSTM network for numerical data. You can then fuse the outputs of the different models using a fusion layer that combines the representations into a joint feature space.\n",
    "\n",
    "-    Define the hyperparameter search space: Specify the hyperparameters that you want to optimize, such as the learning rate, the number of hidden units, the regularization strength, and the dropout rate. You can use a probability distribution to sample the hyperparameters and update the probabilistic model.\n",
    "\n",
    "-    Train and validate the model: Train the machine learning model on the training data and validate its performance on the validation set. Use the objective function (that maximizes the Sharpe ratio of the strategy) to evaluate the model's performance and update the hyperparameters using Bayesian optimization.\n",
    "\n",
    "-    Test the model: Test the final optimized model on a hold-out test set to evaluate its performance in real-world conditions. Monitor its performance over time and adjust the strategy as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aa9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from bayes_opt import BayesianOptimization\n",
    "from hyperopt import fmin, tpe, hp, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a8d94",
   "metadata": {},
   "source": [
    "### Collect and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea70285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Clean text data\n",
    "    data['cleaned_text'] = data['text'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n",
    "    data['cleaned_text'] = data['cleaned_text'].apply(lambda x: x.lower())\n",
    "    data['cleaned_text'] = data['cleaned_text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data['cleaned_text'] = data['cleaned_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    data['cleaned_text'] = data['cleaned_text'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Scale numerical data\n",
    "    scaler = MinMaxScaler()\n",
    "    data['scaled_price'] = scaler.fit_transform(data['price'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate sentiment score\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    data['sentiment_score'] = data['text'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe45ea",
   "metadata": {},
   "source": [
    "### Collect the sentiment data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7235ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "search_term = 'S&P 500'\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=search_term, lang='en').items(1000)\n",
    "tweet_df = pd.DataFrame(columns=['text'])\n",
    "for tweet in tweets:\n",
    "    tweet_df = tweet_df.append({'text': tweet.text}, ignore_index=True)\n",
    "\n",
    "sentiment_data = preprocess_data(tweet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a71bc5",
   "metadata": {},
   "source": [
    "### Create a unified representation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae63a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_data(text_data, numerical_data):\n",
    "    # Merge text and numerical data\n",
    "    fused_data = pd.concat([text_data[['cleaned_text', 'sentiment_score']], numerical_data[['scaled_price']]], axis=1)\n",
    "    \n",
    "    return fused_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2b4ab",
   "metadata": {},
   "source": [
    "### Train a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a829f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, labels):\n",
    "    # Define machine learning model architecture\n",
    "    def build_model(learning_rate, dropout_rate, lstm_units, cnn_filters, cnn_kernel_size, fusion_units):\n",
    "        # Define input layers\n",
    "        text_input = tf.keras.layers.Input(shape=512, name='text_input')\n",
    "        numerical_input = tf.keras.layers.Input(shape=(1,), name='numerical_input')\n",
    "\n",
    "        # Define text input processing layers\n",
    "        text_cnn = tf.keras.layers.Conv1D(filters=cnn_filters, kernel_size=cnn_kernel_size, activation='relu')(text_input)\n",
    "        text_pool = tf.keras.layers.GlobalMaxPooling1D()(text_cnn)\n",
    "\n",
    "        # Define numerical input processing layers\n",
    "        numerical_lstm = tf.keras.layers.LSTM(units=lstm_units, dropout=dropout_rate)(numerical_input)\n",
    "\n",
    "        # Define fusion layer\n",
    "        fusion = tf.keras.layers.Concatenate()([text_pool, numerical_lstm])\n",
    "        fusion = tf.keras.layers.Dense(units=fusion_units, activation='relu')(fusion)\n",
    "        fusion = tf.keras.layers.Dropout(rate=dropout_rate)(fusion)\n",
    "\n",
    "        # Define output layer\n",
    "        output = tf.keras.layers.Dense(units=1, activation='sigmoid')(fusion)\n",
    "\n",
    "        # Define machine learning model\n",
    "        model = tf.keras.models.Model(inputs=[text_input, numerical_input], outputs=output)\n",
    "\n",
    "        # Compile the model with Adam optimizer and binary crossentropy loss\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae5b25",
   "metadata": {},
   "source": [
    "### Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb1866",
   "metadata": {},
   "source": [
    "### Define the hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfe1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -6, -2),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5),\n",
    "    'lstm_units': hp.choice('lstm_units', [64, 128, 256]),\n",
    "    'cnn_filters': hp.choice('cnn_filters', [64, 128, 256]),\n",
    "    'cnn_kernel_size': hp.choice('cnn_kernel_size', [3, 5, 7]),\n",
    "    'fusion_units': hp.choice('fusion_units', [64, 128, 256])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08cd6f",
   "metadata": {},
   "source": [
    "### Define the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def objective(hyperparameters):\n",
    "    # Build the model with the hyperparameters\n",
    "    model = build_model(hyperparameters['learning_rate'], hyperparameters['dropout_rate'], \n",
    "                        hyperparameters['lstm_units'], hyperparameters['cnn_filters'],\n",
    "                        hyperparameters['cnn_kernel_size'], hyperparameters['fusion_units'])\n",
    "\n",
    "    # Train the model on the training data\n",
    "    history = model.fit([X_train_text, X_train_numerical], y_train, epochs=5, \n",
    "                        validation_data=([X_val_text, X_val_numerical], y_val),\n",
    "                        verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation data and compute the Sharpe ratio\n",
    "    y_pred = model.predict([X_val_text, X_val_numerical]).flatten()\n",
    "    returns = y_val * (y_pred - 0.5)\n",
    "    sharpe_ratio = np.sqrt(252) * np.mean(returns) / np.std(returns)\n",
    "\n",
    "    # Return the negative Sharpe ratio as the objective value to minimize\n",
    "    return {'objective': -sharpe_ratio, 'status': 'ok'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093fb06",
   "metadata": {},
   "source": [
    "### Perform the hyperparameter search using Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7634807",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_hyperparameters = fmin(fn=objective, space=hyperparameter_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "#Print the best hyperparameters found by Bayesian optimization\n",
    "print('Best hyperparameters: ', best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682e8f3",
   "metadata": {},
   "source": [
    "### Train the final model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9599af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train, y_train, **best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee59823",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64373e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d9732a",
   "metadata": {},
   "source": [
    "### Calculate predicted labels and probabilities on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e5757",
   "metadata": {},
   "source": [
    "### Calculate Sharpe ratio on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_returns = np.multiply(y_test, test_prices.pct_change().shift(-1))\n",
    "test_strategy_returns = np.multiply(y_pred, test_returns)\n",
    "test_sharpe_ratio = np.sqrt(252) * np.mean(test_strategy_returns) / np.std(test_strategy_returns)\n",
    "print('Test Sharpe ratio: ', test_sharpe_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef082853",
   "metadata": {},
   "source": [
    "\n",
    "This is just a simple example, but the approach can be extended to more complex trading strategies and datasets. I hope you find this approach interesting and useful in your own trading endeavors. Let me know if you have any questions or feedback!\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to modify code to your liking. I hope this helps!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee2338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b77fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c0731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
