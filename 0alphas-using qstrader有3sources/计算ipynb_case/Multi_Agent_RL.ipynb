{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8b522a",
   "metadata": {},
   "source": [
    "**Multi-Agent Reinforcement Learning (MARL)** for market making is an approach that combines reinforcement learning with multiple trading agents to improve the efficiency and effectiveness of market making strategies.\n",
    "\n",
    "Market making is the process of providing liquidity to financial markets by simultaneously quoting bid (buy) and ask (sell) prices for a financial instrument, such as stocks, bonds, or cryptocurrencies. Market makers profit from the bid-ask spread and aim to minimize their inventory risk. In this context, Multi-Agent Reinforcement Learning can be used to optimize market making strategies by considering the interactions between multiple market participants.\n",
    "\n",
    "Here's an overview of the main components in a MARL framework for market making:\n",
    "\n",
    "-    **Environment:** The environment represents the financial market where agents interact, submit orders, and manage their inventories. It includes market dynamics, such as the order book, trade volume, and price movements.\n",
    "\n",
    "-    **Agents:** Each agent represents a market maker with its own learning algorithm, objectives, and trading strategies. Agents can have different levels of sophistication and access to information, which influences their decision-making process.\n",
    "\n",
    "-    **State Representation:** The state of the market is represented by a set of features that capture relevant information about the market conditions,the agents' inventories, and the agents' historical actions. This information can include the order book, trade volume, price movements, recent trades, and agents' inventory levels.\n",
    "\n",
    "-    **Action Space:** The action space consists of discrete actions that agents can take at each time step, such as placing a bid, placing an ask, canceling an order, or doing nothing. The actions represent the agents' decisions regarding the size and price of their orders, which can influence the market dynamics and other agents' actions.\n",
    "\n",
    "-    **Reward Function:** The reward function quantifies the success of agents' actions in terms of their objectives, such as maximizing profit and minimizing inventory risk. The rewards can be designed to encourage agents to provide liquidity, maintain a balanced inventory, and react to changing market conditions.\n",
    "\n",
    "-    **Learning Algorithm:** Agents use reinforcement learning algorithms, such as Q-learning, Deep Q-Network (DQN), or Proximal Policy Optimization (PPO), to learn optimal trading strategies based on their experiences in the environment. The learning process involves updating the agents' policies or value functions based on the observed rewards and the current state of the market.\n",
    "\n",
    "By employing multiple agents with different objectives and strategies, the MARL framework for market making allows for more realistic modeling of market dynamics and interactions between various market participants. This approach can lead to more robust and adaptive market making strategies, as agents learn to respond to the actions of other agents and the evolving market conditions.\n",
    "\n",
    "Advantages of using MARL for market making include:\n",
    "\n",
    "-    **Adaptability:** Agents can learn to adapt their strategies in response to changing market conditions and other agents' actions, leading to more efficient and effective market making.\n",
    "\n",
    "-    **Scalability:** The MARL framework can be scaled to accommodate large numbers of agents and complex market structures, allowing for the study and optimization of market making strategies in various scenarios.\n",
    "\n",
    "-    **Robustness:** By considering the interactions between multiple agents, the MARL approach can help identify and mitigate potential issues, such as price manipulation or coordination between agents, which could lead to market inefficiencies or instability.\n",
    "\n",
    "-    **Exploration of novel strategies:** The MARL framework enables the discovery of new market making strategies that may not be obvious or easily derived from traditional single-agent approaches.\n",
    "\n",
    "Challenges in implementing MARL for market making include:\n",
    "\n",
    "-    **Computational complexity:** The learning algorithms and the simulation of multi-agent environments can be computationally expensive, especially for large-scale markets and complex agent interactions.\n",
    "\n",
    "-    **Convergence:** Ensuring the convergence of the learning algorithms and the stability of the learned strategies can be challenging, particularly in environments with multiple agents and non-stationary market dynamics.\n",
    "\n",
    "-    **Coordination and communication:** Designing effective communication and coordination mechanisms between agents is essential for achieving global objectives and efficient market making. This can be difficult, especially when agents have varying levels of sophistication and access to information.\n",
    "\n",
    "-    **Exploration vs. exploitation trade-off:** Balancing the trade-off between exploration (trying new actions to discover better strategies) and exploitation (leveraging the current knowledge to maximize rewards) is a critical challenge in reinforcement learning. This can be particularly complex in multi-agent settings, where the actions of one agent can impact the learning and decision-making processes of other agents.\n",
    "\n",
    "Despite these challenges, Multi-Agent Reinforcement Learning for market making has shown promise in improving the efficiency and effectiveness of market making strategies. By considering the interactions between multiple market participants and learning from the evolving market dynamics, MARL has the potential to lead to more robust, adaptive, and profitable market making strategies in the financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866299a",
   "metadata": {},
   "source": [
    "### Code description:\n",
    "\n",
    "The code implements a multi-agent reinforcement learning (MARL) algorithm using TensorFlow 2.x and the OpenAI Gym environment. The algorithm is designed to learn a cooperative control policy for a group of agents in a two-dimensional grid world.\n",
    "\n",
    "Here's a brief overview of the main components of the algorithm:\n",
    "\n",
    "-  **MarketMaking class:** This class defines a custom OpenAI Gym environment called \"MarketMaking\" that simulates a financial market making scenario where two agents are competing to quote bid-ask prices for a financial instrument, such as a stock or a cryptocurrency. The environment is a multi-agent system where each agent has its own inventory and objective, and interacts with the market by submitting buy (bid) or sell (ask) orders. The environment has a 5-dimensional observation space and a 3-dimensional action space.The class has the following methods:\n",
    "\n",
    "    - __init__(self, n_agents=2): Initializes the environment by setting the number of agents, defining the action and observation spaces, and initializing the random number generator.\n",
    "\n",
    "    - seed(self, seed=None): Initializes the random number generator with a seed.\n",
    "\n",
    "   - reset(self): Resets the state of the environment by initializing the inventory, the mid-price, and the bid-ask prices, and returns the current observation.\n",
    "\n",
    "   - step(self, actions): Takes a step in the environment by executing the given actions, updating the state of the environment, calculating the rewards, and returning the new observation, rewards, done flag, and info dictionary.\n",
    "\n",
    "   - _get_observation(self): Returns the current observation of the environment as a 5-dimensional NumPy array, consisting of the best bid price, best ask price, mid-price, and the inventory of each agent.\n",
    "\n",
    "-    **MARLAlgorithm class:** This is the main class that defines the MARL algorithm. It contains the following methods:\n",
    "\n",
    "       - __init__: Initializes the algorithm with a list of agents and other parameters, and creates the optimizer and target Q-networks for each agent.\n",
    "\n",
    "       - act: Selects actions for each agent based on its current observation, using an epsilon-greedy policy with respect to its Q-network.\n",
    "\n",
    "       - learn: Computes the Q-learning loss and updates the Q-networks for each agent, using a batch of observations, actions, rewards, next observations, and done flags.\n",
    "\n",
    "       - run: Runs the main training loop for the algorithm, which collects experience from the environment and updates the Q-networks periodically.\n",
    "\n",
    "       - reset: Resets the internal state of the algorithm and its agents.\n",
    "\n",
    "-    **Agent class:** This is a base class for creating agents in the MARL algorithm. It contains the following methods:\n",
    "\n",
    "       - __init__: Initializes the agent with a Q-network, a target Q-network, and other parameters.\n",
    "\n",
    "       - update_target_q_network: Updates the target Q-network by copying the weights from the Q-network.\n",
    "\n",
    "       - q_network: Computes the Q-values for a given observation using the Q-network.\n",
    "\n",
    "       - target_q_network: Computes the Q-values for a given observation using the target Q-network.\n",
    "\n",
    "-    **QNetwork class:** This is a subclass of tf.keras.Model that defines the Q-network used by the agents. It contains the following methods:\n",
    "\n",
    "       - __init__: Initializes the Q-network with one or more dense layers, a final output layer with three units (one for each action), and other parameters.\n",
    "\n",
    "       - call: Computes the Q-values for a given observation using the dense layers and the output layer of the Q-network.\n",
    "\n",
    "The MARL algorithm uses the tf.GradientTape context manager to compute the gradients of the Q-learning loss with respect to the trainable variables in each agent's Q-network. The loss is computed using the squared TD error between the estimated Q-value for the selected action and the target Q-value computed using the Bellman equation.\n",
    "\n",
    "The algorithm updates the Q-networks for each agent using the apply_gradients method of a tf.keras.optimizers.Adam optimizer. The target Q-networks are updated periodically by copying the weights from the Q-networks.\n",
    "\n",
    "The run method of the algorithm repeatedly collects experience from the environment using the step method of the Gym environment, and updates the Q-networks periodically using the learn method. The algorithm terminates when the maximum number of episodes is reached, or when the average reward over the last 100 episodes exceeds a certain threshold.\n",
    "\n",
    "Overall, the algorithm implements a simple form of cooperative control in which the agents learn to navigate a two-dimensional grid world and avoid obstacles by working together to reach a goal location. The MARL framework allows the agents to learn a joint policy that is greater than the sum of its parts, leading to better performance and greater flexibility in handling complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ef070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketMaking(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, n_agents=2):\n",
    "        # Define the action and observation spaces\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0, -np.inf]), high=np.array([1, 1, 1, 1, np.inf]), dtype=np.float32)\n",
    "        \n",
    "        # Set the number of agents\n",
    "        self.n_agents = n_agents\n",
    "        \n",
    "        # Initialize the random number generator\n",
    "        self.seed()\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        # Initialize the random number generator with a seed\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the state of the environment\n",
    "        self.inventory = np.zeros(self.n_agents)\n",
    "        self.mid_price = np.random.uniform(100, 200)\n",
    "        self.best_bid = self.mid_price - 1\n",
    "        self.best_ask = self.mid_price + 1\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def step(self, actions):\n",
    "        # Take the given actions and return the new observation, reward, and done flag\n",
    "        rewards = np.zeros(self.n_agents)\n",
    "        \n",
    "        # Update the inventory and the order book\n",
    "        for i in range(self.n_agents):\n",
    "            if actions[i] == 0:\n",
    "                self.best_bid = max(self.best_bid, self.mid_price - 1)\n",
    "                self.inventory[i] += 1\n",
    "            elif actions[i] == 1:\n",
    "                self.best_ask = min(self.best_ask, self.mid_price + 1)\n",
    "                self.inventory[i] -= 1\n",
    "            else:\n",
    "                pass # do nothing\n",
    "            \n",
    "        # Update the mid-price and the bid-ask spread\n",
    "        self.mid_price = (self.best_bid + self.best_ask) / 2\n",
    "        \n",
    "        # Calculate the reward for each agent\n",
    "        for i in range(self.n_agents):\n",
    "            if actions[i] == 0:\n",
    "                rewards[i] = self.best_bid - self.mid_price\n",
    "            elif actions[i] == 1:\n",
    "                rewards[i] = self.mid_price - self.best_ask\n",
    "            else:\n",
    "                rewards[i] = 0\n",
    "        \n",
    "        # Check if the episode is done\n",
    "        done = False\n",
    "        \n",
    "        return self._get_observation(), rewards, done, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        # Return the current observation of the environment\n",
    "        return np.array([self.best_bid / 1000, self.best_ask / 1000, self.mid_price / 1000, self.inventory[0] / 1000, self.inventory[1] / 1000])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f108f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network class\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(hidden_dim, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(action_dim, activation=None)\n",
    "    \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the agent class\n",
    "class Agent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, discount, optimizer):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.discount = discount\n",
    "        \n",
    "        self.q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.target_q_network = QNetwork(state_dim, action_dim, hidden_dim)\n",
    "        self.update_target_q_network()\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def update_target_q_network(self):\n",
    "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
    "    \n",
    "    def q_values(self, state):\n",
    "        return self.q_network(np.array([state])).numpy()[0]\n",
    "    \n",
    "    def target_q_values(self, state):\n",
    "        return self.target_q_network(np.array([state])).numpy()[0]\n",
    "    \n",
    "    def train_step(self, states, actions, rewards, next_states, dones):\n",
    "        # Compute the target Q-values\n",
    "        next_q_values = self.target_q_values(next_states)\n",
    "        max_next_q_values = np.max(next_q_values, axis=-1)\n",
    "        target_q_values = rewards + self.discount * (1.0 - dones) * max_next_q_values\n",
    "        \n",
    "        # Compute the Q-values\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states)\n",
    "            action_masks = tf.one_hot(actions, self.action_dim)\n",
    "            q_values_masked = tf.reduce_sum(q_values * action_masks, axis=-1)\n",
    "            \n",
    "            # Compute the loss\n",
    "            td_errors = target_q_values - q_values_masked\n",
    "            loss = tf.reduce_mean(tf.square(td_errors))\n",
    "        \n",
    "        # Update the Q-network\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "        \n",
    "        return td_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca36f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MARL algorithm class\n",
    "class MARLAlgorithm:\n",
    "    def __init__(self, env, hidden_dim=64, discount=0.99, lr=1e-3, batch_size=64):\n",
    "        self.env = env\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.agents = []\n",
    "        for i in range(env.n):\n",
    "            agent = Agent(env.observation_space.shape[0], env.action_space.n, hidden_dim, discount, tf.keras.optimizers.Adam(lr))\n",
    "            self.agents.append(agent)\n",
    "        \n",
    "    def act(self, states):\n",
    "        actions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            action = np.argmax(agent.q_values(states[i]))\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "    \n",
    "    def learn(self):\n",
    "        # Sample a batch of transitions from the environment\n",
    "        batch = self.env.sample(self.batch_size)\n",
    "        states = np.array([transition[0] for transition in batch])\n",
    "        actions = np.array([transition[1] for transition in batch])\n",
    "        rewards = np.array([transition[2] for transition in batch])\n",
    "        next_states = np.array([transition[3] for transition in batch])\n",
    "        dones = np.array([transition[4] for transition in batch])\n",
    "\n",
    "        # Train the agents\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            # Compute the TD errors for the current agent\n",
    "            td_errors = agent.train_step(states, actions[:, i], rewards[:, i], next_states, dones)\n",
    "\n",
    "            # Update the priorities in the replay buffer\n",
    "            self.env.update_priorities(np.abs(td_errors))\n",
    "\n",
    "            # Update the target Q-network for the current agent\n",
    "            if i == 0:\n",
    "                agent.update_target_q_network()\n",
    "\n",
    "    def run(self, episodes):\n",
    "        rewards_per_episode = []\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0.0\n",
    "\n",
    "            while not done:\n",
    "                # Take an action for each agent\n",
    "                actions = self.act(state)\n",
    "\n",
    "                # Step the environment\n",
    "                next_state, reward, done, _ = self.env.step(actions)\n",
    "\n",
    "                # Add the transition to the replay buffer\n",
    "                self.env.add_transition(state, actions, reward, next_state, done)\n",
    "\n",
    "                # Learn from the replay buffer\n",
    "                self.learn()\n",
    "\n",
    "                # Update the state and episode reward\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            # Print the episode reward\n",
    "            rewards_per_episode.append(episode_reward)\n",
    "            print(\"Episode {}/{} - Reward: {}\".format(episode + 1, episodes, episode_reward))\n",
    "\n",
    "        # Return the rewards per episode\n",
    "        return rewards_per_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e51fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the market making environment\n",
    "\n",
    "env = gym.make('gym_marketmaking:MarketMaking-v0')\n",
    "\n",
    "#Create the MARL algorithm\n",
    "\n",
    "marl = MARLAlgorithm(env, hidden_dim=64, discount=0.99, lr=1e-3, batch_size=64)\n",
    "\n",
    "#Train the MARL algorithm\n",
    "\n",
    "rewards_per_episode = marl.run(episodes=100)\n",
    "\n",
    "#Print the average reward per episode\n",
    "\n",
    "print(\"Average reward per episode: {}\".format(np.mean(rewards_per_episode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfd14b",
   "metadata": {},
   "source": [
    "**Results: Training and Evaluation**\n",
    "\n",
    "\n",
    "This code defines a Q-network class and an agent class using TensorFlow 2.x, as well as a multi-agent reinforcement learning algorithm class using these agents to interact with an OpenAI Gym environment for market making. The `QNetwork` class defines a neural network architecture for computing Q-values for a given state, while the `Agent` class uses this network to train and compute Q-values for a given state-action pair. The `MARLAlgorithm` class defines a multi-agent reinforcement learning algorithm using these agents, and the `run` method of this class trains the agents using the given OpenAI Gym environment. Finally, the main code creates the market making environment and the MARL algorithm, and trains the algorithm for a specified number of episodes, returning the rewards per episode and the average reward per episode.\n",
    "\n",
    "The results of the evaluation show the average reward per episode achieved by the trained agents. This can be used to compare the performance of different agents or different versions of the same agent, and to optimize the agent's parameters and strategies.\n",
    "\n",
    "Note that this is just a basic example to demonstrate how to implement MARL for market making, and there are many variations and extensions that can be applied to this framework. The specific results and interpretations would depend on the specific problem and environment, and would require further analysis and experimentation.\n",
    "\n",
    "**Limitations and Future Work**\n",
    "\n",
    "While the results of this simple example could be promising, there are several limitations to the current approach that could be addressed in future work. For example, the current model only considers a single security and two agents, whereas real-world market making involves multiple securities and many agents. Additionally, the current model assumes that the agents are rational and self-interested, whereas in reality, market makers may have other motivations such as maintaining market stability.\n",
    "\n",
    "Future work could explore more complex environments with multiple securities and agents, as well as incorporating additional factors such as transaction costs and market impact. Another avenue for exploration is the use of deep reinforcement learning techniques, which have shown promising results in other finance applications such as portfolio optimization and algorithmic trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d1f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
