{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3ebe5d",
   "metadata": {},
   "source": [
    "Some examples of how to extract entropy and microstructural features from financial market data:\n",
    "\n",
    "### Entropy Features:\n",
    "\n",
    "-    **Shannon entropy**: This measures the degree of randomness or uncertainty in a probability distribution. In finance, Shannon entropy can be used to measure the uncertainty of price changes, trading volumes, or other market variables. To calculate Shannon entropy, you first need to calculate the probability distribution of the variable of interest. This can be done using techniques such as kernel density estimation or histogramming. Once you have the probability distribution, you can calculate Shannon entropy using the formula: H = - sum(p * log(p)), where p is the probability of each value in the distribution.\n",
    "\n",
    "-    **Approximate entropy**: This is a measure of the complexity of a time series. It can be used to identify patterns or anomalies in financial market data. To calculate approximate entropy, you first need to define a tolerance level and a length of comparison. The tolerance level determines how similar two data points need to be in order to be considered \"close\". The length of comparison determines how many data points are compared. Once you have defined these parameters, you can calculate approximate entropy using the formula: ApEn(m, r, N) = (phi(m+1) - phi(m)), where m is the length of comparison, r is the tolerance level, N is the length of the time series, and phi() is a function that calculates the average logarithm of the number of sequences in the time series that are similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2dc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f0c2d5",
   "metadata": {},
   "source": [
    "### Shannon entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea50428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon entropy: 2.685421325303823\n"
     ]
    }
   ],
   "source": [
    "# Generate a random price series\n",
    "price_series = np.random.normal(100, 10, 1000)\n",
    "\n",
    "# Calculate the probability distribution of price changes\n",
    "price_changes = np.diff(price_series) / price_series[:-1]\n",
    "p, bins = np.histogram(price_changes, bins='auto', density=True)\n",
    "\n",
    "# Calculate Shannon entropy\n",
    "shannon_entropy = entropy(p)\n",
    "print(\"Shannon entropy:\", shannon_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528cff4",
   "metadata": {},
   "source": [
    "### Approximate entropy\n",
    "In this code snippet, the 'apen' function is defined manually using NumPy and a for loop. The 'apen' function takes three arguments: the financial time series data, the embedding dimension (i.e., the number of data points used to define a state), and the tolerance value (i.e., the maximum distance between two similar states).\n",
    "\n",
    "The code then generates a random price series of length 1000 using NumPy's 'random.normal' function. The price series is assumed to have a mean of 100 and a standard deviation of 10.\n",
    "\n",
    "Finally, the code calculates the ApEn of the price series using the manually defined 'apen' function. The embedding dimension is set to 5, and the tolerance value is set to 0.2. The ApEn value is then printed to the console.\n",
    "\n",
    "In this implementation of the 'apen' function, the data vectors are reshaped to be 2-dimensional arrays with a single column, and then the distance matrix is calculated using the absolute difference between the data vectors. The 'np.max' function is then used to find the maximum distance between each pair of data vectors along the 0th axis (i.e., the rows), and the resulting array is compared to the tolerance value to determine the number of similar patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92fd6e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate entropy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define the ApEn function\n",
    "def apen(x, m, r):\n",
    "    \"\"\"\n",
    "    Calculate the Approximate Entropy (ApEn) of a time series.\n",
    "    x: the time series data\n",
    "    m: the embedding dimension\n",
    "    r: the tolerance value\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    phi = np.zeros((N - m + 1, 1))\n",
    "    for i in range(N - m + 1):\n",
    "        # Define the data vectors\n",
    "        xmi = x[i:i+m].reshape(-1, 1)\n",
    "        xmj = x[i+1:i+m+1].reshape(-1, 1)\n",
    "        # Calculate the distance matrix\n",
    "        C = np.abs(xmj - xmi.T)\n",
    "        # Count the number of similar patterns\n",
    "        phi[i] = np.sum(np.max(C, axis=1) <= r) / (N - m + 1)\n",
    "    ApEn = np.mean(phi)\n",
    "    return ApEn\n",
    "\n",
    "# Generate a random price series\n",
    "price_series = np.random.normal(100, 10, 1000)\n",
    "\n",
    "# Calculate the ApEn of the price series\n",
    "tolerance = 0.2\n",
    "embedding_dimension = 5\n",
    "approx_entropy = apen(price_series, embedding_dimension, tolerance)\n",
    "print(\"Approximate entropy:\", approx_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334314e",
   "metadata": {},
   "source": [
    "If the approximate entropy is calculated as 0, it means that the price series is highly predictable or regular. In other words, there is very little randomness or complexity in the data. This can happen if the data is generated by a simple model or if there is a high degree of autocorrelation in the data.\n",
    "\n",
    "To confirm this, you can try generating a more complex or unpredictable price series and see if the approximate entropy value changes. You can also try adjusting the tolerance and length_of_comparison parameters to see if that affects the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d243b6",
   "metadata": {},
   "source": [
    "### Microstructural Features:\n",
    "\n",
    "-    **Bid-ask spread**: This is the difference between the highest price a buyer is willing to pay (the bid) and the lowest price a seller is willing to accept (the ask). The bid-ask spread is a measure of market liquidity and can be used to identify trading opportunities. To extract bid-ask spread data, you can use order book data provided by exchanges.\n",
    "\n",
    "-    **Trading volume**: This is the total number of shares or contracts that are traded during a given time period. Trading volume is a measure of market activity and can be used to identify trends or changes in market sentiment. To extract trading volume data, you can use trade data provided by exchanges.\n",
    "\n",
    "-    **Order book depth**: This is the total number of shares or contracts that are available at each price level in the order book. Order book depth is a measure of market depth and can be used to identify levels of support or resistance. To extract order book depth data, you can use order book data provided by exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6a171",
   "metadata": {},
   "source": [
    "The Roll Model and Corwin and Schultz are two well-known models for estimating the bid-ask spread in financial markets.\n",
    "\n",
    "The Roll Model, developed by Richard Roll in 1984, is a widely used model for estimating the bid-ask spread in equity markets. The model assumes that the spread is proportional to the volatility of the stock's price, and inversely proportional to the stock's trading volume. The formula for the Roll Model is: spread = a + b * volatility + c * volume, where a, b, and c are coefficients that can be estimated using regression analysis. The Roll Model is widely used because it is simple and easy to implement, and it has been shown to provide accurate estimates of the bid-ask spread in many different markets.\n",
    "\n",
    "Corwin and Schultz is a more recent model, developed in 2012 by Joel Corwin and Paul Schultz. The Corwin and Schultz model is designed to estimate the bid-ask spread in markets where the bid and ask prices are not observed directly, but can be inferred from trade data. The model assumes that the spread is proportional to the difference between the high and low prices of a stock over a given time period. The formula for the Corwin and Schultz model is: spread = a + b * log(d), where a and b are coefficients that can be estimated using regression analysis, and d is the number of trading days over which the high and low prices are observed. The Corwin and Schultz model has been shown to provide accurate estimates of the bid-ask spread in markets where the bid and ask prices are not directly observable, such as the foreign exchange market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83ab72",
   "metadata": {},
   "source": [
    "### Roll Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Calculate daily returns and volatility\n",
    "data['return'] = np.log(data['close']).diff()\n",
    "data['volatility'] = data['return'].rolling(window=10).std()\n",
    "\n",
    "# Calculate trading volume\n",
    "data['volume'] = data['volume'].rolling(window=10).mean()\n",
    "\n",
    "# Estimate coefficients using regression analysis\n",
    "X = data[['volatility', 'volume']]\n",
    "y = data['bid_ask_spread']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "a = results.params[0]\n",
    "b = results.params[1]\n",
    "c = results.params[2]\n",
    "\n",
    "# Calculate bid-ask spread using Roll Model\n",
    "data['spread'] = a + b * data['volatility'] + c * data['volume']\n",
    "print(\"Bid-ask spread using Roll Model:\\n\", data['spread'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9067237",
   "metadata": {},
   "source": [
    "### Corwin and Schultz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd638997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Calculate high-low range and trading days\n",
    "data['range'] = data['high'] - data['low']\n",
    "data['trading_days'] = data['date'].diff().dt.days.fillna(0)\n",
    "\n",
    "# Estimate coefficients using regression analysis\n",
    "X = np.log(data['trading_days']).values.reshape(-1, 1)\n",
    "y = data['range']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "a = results.params[0]\n",
    "b = results.params[1]\n",
    "\n",
    "# Calculate bid-ask spread using Corwin and Schultz\n",
    "data['spread'] = a + b * np.log(30)\n",
    "print(\"Bid-ask spread using Corwin and Schultz:\\n\", data['spread'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69178690",
   "metadata": {},
   "source": [
    "### Hasbrouck’s Lambda\n",
    "Hasbrouck’s Lambda is a microstructure measure that estimates the price impact of a trade. It is based on the idea that the price change resulting from a trade is a function of the trade size and the level of market liquidity. In essence, it measures the market’s sensitivity to trade size, and it is commonly used in transaction cost analysis.\n",
    "\n",
    "The formula for Hasbrouck’s Lambda is as follows:\n",
    "\n",
    "lambda = delta_p / delta_q\n",
    "\n",
    "Where:\n",
    "\n",
    "-    delta_p: the change in the mid-price of the asset\n",
    "-    delta_q: the trade size\n",
    "\n",
    "To implement Hasbrouck’s Lambda in Python, you can use the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trade data\n",
    "trades = pd.read_csv(\"trades.csv\")\n",
    "\n",
    "# Calculate mid-price\n",
    "trades['mid_price'] = (trades['bid_price'] + trades['ask_price']) / 2\n",
    "\n",
    "# Calculate price change and trade size\n",
    "trades['delta_p'] = trades['mid_price'].diff()\n",
    "trades['delta_q'] = trades['volume']\n",
    "\n",
    "# Calculate Hasbrouck’s Lambda\n",
    "trades['lambda'] = trades['delta_p'] / trades['delta_q']\n",
    "\n",
    "print(\"Hasbrouck’s Lambda:\\n\", trades['lambda'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d41ab2",
   "metadata": {},
   "source": [
    "The above examples are simplified and may need to be adapted to the specific data and use case. Additionally, the performance of the models may vary depending on the quality and characteristics of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
