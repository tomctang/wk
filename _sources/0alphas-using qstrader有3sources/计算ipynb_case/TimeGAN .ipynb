{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86659794",
   "metadata": {},
   "source": [
    "## Generating Synthetic Financial Data using GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaeead7",
   "metadata": {},
   "source": [
    "Implementation of the TimeGAN architecture in TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, seq_len, n_features, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder model\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(64, input_shape=(seq_len, n_features)),\n",
    "            tf.keras.layers.Dense(latent_dim),\n",
    "        ])\n",
    "        \n",
    "        # Decoder model\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(64, input_shape=(latent_dim,)),\n",
    "            tf.keras.layers.RepeatVector(seq_len),\n",
    "            tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features)),\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Encode input x\n",
    "        encoded = self.encoder(x)\n",
    "        # Decode encoded input\n",
    "        decoded = self.decoder(encoded)\n",
    "        # Return decoded input\n",
    "        return decoded\n",
    "\n",
    "\n",
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self, seq_len, n_features):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # Bidirectional LSTM model\n",
    "        self.rnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))\n",
    "        self.fc = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))\n",
    "        self.activation = tf.keras.activations.sigmoid\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Pass input x through the RNN\n",
    "        x = self.rnn(x)\n",
    "        # Apply fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # Apply activation function\n",
    "        x = self.activation(x)\n",
    "        # Return output\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, seq_len, latent_dim, n_features):\n",
    "        super(Generator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_features = n_features\n",
    "        \n",
    "        # LSTM model\n",
    "        self.rnn = tf.keras.layers.LSTM(64, input_shape=(seq_len, latent_dim), return_sequences=True)\n",
    "        self.fc = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Pass input x through the LSTM\n",
    "        x = self.rnn(x)\n",
    "        # Apply fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # Return output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d05d3",
   "metadata": {},
   "source": [
    "The **TimeGAN model** is a generative model that can be used to generate synthetic time series data.\n",
    "\n",
    "The code defines three different models: **Autoencoder, Discriminator, and Generator**. The Autoencoder is used to reconstruct the input data, the Discriminator is used to distinguish between real and fake data, and the Generator is used to generate new synthetic data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGAN(tf.keras.Model):\n",
    "    def __init__(self, seq_len, n_features, latent_dim):\n",
    "        super(TimeGAN, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Instantiate models\n",
    "        self.generator = Generator(seq_len, latent_dim, n_features)\n",
    "        self.discriminator = Discriminator(seq_len, n_features)\n",
    "        self.autoencoder = Autoencoder(seq_len, n_features, latent_dim)\n",
    "        \n",
    "        # Define optimizers\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        \n",
    "        # Define loss functions\n",
    "        self.reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "        self.adversarial_loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "        \n",
    "    def compile(self, reconstruction_weight=1.0, adversarial_weight=1.0):\n",
    "        super(TimeGAN, self).compile()\n",
    "        self.reconstruction_weight = reconstruction_weight\n",
    "        self.adversarial_weight = adversarial_weight\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        real_x = x\n",
    "\n",
    "        # Train generator\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            z = tf.random.normal((tf.shape(real_x)[0], self.seq_len, self.latent_dim))\n",
    "            fake_x = self.generator(z)\n",
    "            fake_discriminator_output = self.discriminator(fake_x)\n",
    "            gen_loss = self.adversarial_weight * self.adversarial_loss_fn(tf.ones_like(fake_discriminator_output), fake_discriminator_output)\n",
    "            reconstructed_x = self.autoencoder(fake_x)\n",
    "            recon_loss = self.reconstruction_weight * self.reconstruction_loss_fn(real_x, reconstructed_x)\n",
    "            gen_loss += recon_loss\n",
    "        gen_gradients = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
    "\n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            real_discriminator_output = self.discriminator(real_x)\n",
    "            fake_discriminator_output = self.discriminator(fake_x)\n",
    "            disc_loss = self.adversarial_weight * (\n",
    "                self.adversarial_loss_fn(tf.ones_like(real_discriminator_output), real_discriminator_output) +\n",
    "                self.adversarial_loss_fn(tf.zeros_like(fake_discriminator_output), fake_discriminator_output)\n",
    "            )\n",
    "        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.discriminator_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n",
    "\n",
    "        return {\"gen_loss\": gen_loss, \"disc_loss\": disc_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082aeff",
   "metadata": {},
   "source": [
    "In the **compile** method, we are simply initializing the reconstruction_weight and adversarial_weight attributes of the class with the given values.\n",
    "\n",
    "In the **train_step** method, we first define real_x as the input data x. We then train the generator by first generating fake data by passing random noise z through the generator, and computing the output of the discriminator on the fake data. We compute the generator loss as a weighted sum of the adversarial loss and the reconstruction loss, where the reconstruction loss measures the difference between the real and reconstructed data. We then compute the gradients of the generator loss with respect to the generator's trainable variables, and apply these gradients using the generator optimizer.\n",
    "\n",
    "We then train the discriminator by computing the output of the discriminator on the real and fake data, and computing the adversarial loss as a weighted sum of the losses on the real and fake data. We then compute the gradients of the discriminator loss with respect to the discriminator's trainable variables, and apply these gradients using the discriminator optimizer.\n",
    "\n",
    "Finally, we return a dictionary containing the generator loss and discriminator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6faab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the synthetic data\n",
    "seq_len = 50  # sequence length\n",
    "n_features = 1  # number of features in the time series\n",
    "latent_dim = 10  # dimension of the latent space\n",
    "n_samples = 1000  # number of samples to generate\n",
    "\n",
    "# Initialize the TimeGAN model\n",
    "model = TimeGAN(seq_len=seq_len, n_features=n_features, latent_dim=latent_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(reconstruction_weight=1.0, adversarial_weight=1.0)\n",
    "\n",
    "# Generate synthetic data\n",
    "z = tf.random.normal((n_samples, seq_len, latent_dim))\n",
    "generated_data = model.generator(z).numpy()\n",
    "\n",
    "# Reshape the generated data to match the desired shape\n",
    "generated_data = np.reshape(generated_data, (n_samples, seq_len, n_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c539c9",
   "metadata": {},
   "source": [
    "Now, to generate synthetic data we first define the parameters for the synthetic data we want to generate, including the sequence length, number of features, latent dimension, and number of samples. We then initialize a TimeGAN model with these parameters, compile it, and generate synthetic data using the generator component of the model.\n",
    "\n",
    "Finally, we reshape the generated data to match the desired shape (i.e., a 3D tensor with shape (n_samples, seq_len, n_features))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
